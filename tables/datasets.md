# üìö Demonstration Datasets

| Name | Robot | Type | Data | Control | Lang Cond. | Env |
|------|-------|------|------|---------|-------------|-----|
| [LIBERO](https://libero-project.github.io/) | Franka Panda | Simulated | Manipulation | Low-Level | ‚úÖ | tabletop |
| [CALVIN](http://calvin.cs.uni-freiburg.de/) | Franka Panda | Simulated | Manipulation | Low-Level | ‚úÖ | tabletop |
| [VLA-Bench](https://vlabench.github.io/) | Franka Panda | Simulated | Manipulation+Reasoning | Low-Level | ‚úÖ | tabletop |
| [ALOHA](https://tonyzhaozh.github.io/aloha/) | Aloha | Real world | Bimanual manipulation | Low-Level | ‚ùå | mobile |
| [LangR](https://llm-rl.github.io/) | Fetch | Simulated | Manipulation | Magic-Grasp | ‚úÖ | mobile |
| [SimplerEnv](https://simpler-env.github.io/) | Google Robot, WidowX | Simulated | Manipulation | Low-Level | ‚úÖ | tabletop |
| [RL-Bench](https://sites.google.com/view/rlbench) | Franka Panda | Simulated | Low-level Manipulation | Low-Level | ‚úÖ | tabletop |

<details>
<summary>üìÑ Papers using LIBERO (click to expand)</summary>

- [OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/abs/2406.09246)
- [Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success](https://arxiv.org/abs/2502.19645)
- [FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies](https://arxiv.org/abs/2509.04996)
- [VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model](https://arxiv.org/abs/2509.09372)
- [MolmoAct: Action Reasoning Models that can Reason in Space](https://arxiv.org/abs/2508.07917)
- [SmolVLA: A vision-language-action model for affordable and efficient robotics](https://arxiv.org/abs/2506.01844)
- [InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning](https://arxiv.org/abs/2505.13888)
- [villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models](https://arxiv.org/abs/2507.23682)

</details>

<details>
<summary>üìÑ Papers using CALVIN (click to expand)</summary>

- [FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies](https://arxiv.org/abs/2509.04996)
- [VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model](https://arxiv.org/abs/2509.09372)

</details>

<details>
<summary>üìÑ Papers using VLA-Bench (click to expand)</summary>

- [Open Paper](https://vlabench.github.io/)

</details>

<details>
<summary>üìÑ Papers using ALOHA (click to expand)</summary>

- [Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware](https://arxiv.org/abs/2304.13705)
- [Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation](https://arxiv.org/abs/2401.02117)
- [FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies](https://arxiv.org/abs/2509.04996)

</details>

<details>
<summary>üìÑ Papers using LangR (click to expand)</summary>

- [From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons](https://arxiv.org/abs/2412.08442)

</details>

<details>
<summary>üìÑ Papers using SimplerEnv (click to expand)</summary>

- [CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation](https://arxiv.org/abs/2411.19650)
- [FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies](https://arxiv.org/abs/2509.04996)
- [ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models](https://arxiv.org/abs/2409.15250)
- [villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models](https://arxiv.org/abs/2507.23682)

</details>

<details>
<summary>üìÑ Papers using RL-Bench (click to expand)</summary>

- [MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation](https://arxiv.org/abs/2503.20384)

</details>
